

<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>Try RAG with LlamaIndex - </title>

  <meta name="description" content="Motivation
In this post where I tested Chatbot UI, I mentioned that one of my future challenges is to work with RAG (Retrieval Augmented Generation). In this post, I summarized how to achieve RAG using LlamaIndex.
Actually, I tried RAG using Langchain late last year. Since then, I have heard a lot of keywords with LlamaIndex, so I decided to realize RAG using LlamaIndex this time."><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "akenji\u0027s lab",
    
    "url": "https:\/\/akenji3.github.io\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/akenji3.github.io\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/akenji3.github.io\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/akenji3.github.io\/en\/post\/20240525_llamaindex_rag\/",
          "name": "Try rag with llama index"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : ""
  },
  "headline": "Try RAG with LlamaIndex",
  "description" : "Motivation In this post where I tested Chatbot UI, I mentioned that one of my future challenges is to work with RAG (Retrieval Augmented Generation). In this post, I summarized how to achieve RAG using LlamaIndex.\nActually, I tried RAG using Langchain late last year. Since then, I have heard a lot of keywords with LlamaIndex, so I decided to realize RAG using LlamaIndex this time.\n",
  "inLanguage" : "en",
  "wordCount":  1105 ,
  "datePublished" : "2024-05-25T00:00:00\u002b00:00",
  "dateModified" : "2024-05-25T00:00:00\u002b00:00",
  "image" : "https:\/\/akenji3.github.io\/img\/avatar-icon.png",
  "keywords" : [ "" ],
  "mainEntityOfPage" : "https:\/\/akenji3.github.io\/en\/post\/20240525_llamaindex_rag\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/akenji3.github.io\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/akenji3.github.io\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>


<meta property="og:title" content="Try RAG with LlamaIndex" />
<meta property="og:description" content="Motivation
In this post where I tested Chatbot UI, I mentioned that one of my future challenges is to work with RAG (Retrieval Augmented Generation). In this post, I summarized how to achieve RAG using LlamaIndex.
Actually, I tried RAG using Langchain late last year. Since then, I have heard a lot of keywords with LlamaIndex, so I decided to realize RAG using LlamaIndex this time.">
<meta property="og:image" content="https://akenji3.github.io/img/avatar-icon.png" />
<meta property="og:url" content="https://akenji3.github.io/en/post/20240525_llamaindex_rag/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="akenji&#39;s lab" />

  <meta name="twitter:title" content="Try RAG with LlamaIndex" />
  <meta name="twitter:description" content="Motivation
In this post where I tested Chatbot UI, I mentioned that one of my future challenges is to work with RAG (Retrieval Augmented Generation). In this post, I summarized how to achieve RAG …">
  <meta name="twitter:image" content="https://akenji3.github.io/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='https://akenji3.github.io/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.147.1">
  <link rel="alternate" href="https://akenji3.github.io/en/index.xml" type="application/rss+xml" title="akenji&#39;s lab"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.6.0/css/all.css" integrity="sha384-h/hnnw1Bi4nbpD6kE7nYfCXzovi622sY5WBxww8ARKwpdLj5kUWjRuyiXaD1U2JT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="https://akenji3.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://akenji3.github.io/css/highlight.min.css" /><link rel="stylesheet" href="https://akenji3.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-TGXWYJXF48"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TGXWYJXF48');
        }
      </script>
  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://akenji3.github.io/en/">akenji&#39;s lab</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/en/">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="/en/page/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Categories" href="/en/categories">Categories</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/en/tags">Tags</a>
            </li>
          
        

        
          
            <li>
              
                
                  <a href="https://akenji3.github.io/post/20240525_llamaindex_rag/">ja</a>
                
              
            </li>
          
        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="akenji&#39;s lab" href="https://akenji3.github.io/en/">
            <img class="avatar-img" src="https://akenji3.github.io/img/avatar-icon.png" alt="akenji&#39;s lab" />
           
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>Try RAG with LlamaIndex</h1>
              
              
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;Posted on May 25, 2024
  
  
  
  
    
      &nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;
    
  
  &nbsp;&bull;&nbsp;Other languages: <a href="https://akenji3.github.io/post/20240525_llamaindex_rag/" lang="ja">ja</a>
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <h2 id="motivation">Motivation</h2>
<p>In <a href="https://akenji3.github.io/en/post/20240506_llamaserver/">this post</a> where I tested Chatbot UI, I mentioned that one of my future challenges is to work with RAG (Retrieval Augmented Generation). In this post, I summarized how to achieve RAG using LlamaIndex.</p>
<p>Actually, I tried RAG using Langchain late last year. Since then, I have heard a lot of keywords with LlamaIndex, so I decided to realize RAG using LlamaIndex this time.</p>
<h2 id="sources">Sources</h2>
<ol>
<li><a href="https://tech.dentsusoken.com/entry/2024/01/22/LlamaIndex-RAG">How to execute RAG in a local environment using LlamaIndex</a>  The content of the article (including code and additional information for RAG) was used verbatim. The article was written in January of this year and was written in the code before LlamaIndex was upgraded, so that part needs to be changed.</li>
<li><a href="https://note.com/alexweberk/n/n3cffc010e9e9">Tried RAG with LangChain using Elyza 7b</a>  An article I referred to when trying RAG last year during the year-end vacations.</li>
<li>[ImportError: cannot import name &lsquo;SimpleDirectoryReader&rsquo; from &rsquo;llama_index&rsquo; (unknown location)](<a href="https://qiita.com/miyamotok0105/items/9f">https://qiita.com/miyamotok0105/items/9f</a> 5d1fc8b92e3447a75f)  The library has been reorganized due to the version change of llama-index.</li>
<li><a href="https://docs.llamaindex.ai/en/stable/examples/data_connectors/WebPageDemo/">Web Page Reader</a>  In source 1, text data was used as data for the embedded model, It contains hints for using Web page information as the embedding model.</li>
</ol>
<h2 id="implementation-in-llamaindex">Implementation in LlamaIndex</h2>
<h4 id="dockerfile">Dockerfile</h4>
<p>The first half of the Dockerfile is based on Source 1, and the part that incorporates the python library is largely taken from the Dockerfile used in <a href="https://akenji3.github.io/en/post/20240503_llama-cpp-python/">this post</a>. The first half of the file is based on the source 1.</p>
<p>The two lines before &ldquo;pip install llama-index&rdquo; are necessary if llama_index is v0.10.0 or higher.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-dockerfile" data-lang="dockerfile"><span class="line"><span class="cl"><span class="k">FROM</span><span class="s"> nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Set bash as the default shell</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">ENV</span> <span class="nv">SHELL</span><span class="o">=</span>/bin/bash<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Build with some basic utilities</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> apt update <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	<span class="o">&amp;&amp;</span> apt install -y <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        wget <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        bzip2 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        git <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        git-lfs <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        curl <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        unzip <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        file <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        xz-utils <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        sudo <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        python3 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        python3-pip <span class="o">&amp;&amp;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        apt-get autoremove -y <span class="o">&amp;&amp;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        apt-get clean <span class="o">&amp;&amp;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        rm -rf /usr/local/src/*<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># alias python=&#39;python3&#39;</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> ln -s /usr/bin/python3 /usr/bin/python<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> pip install --upgrade pip setuptools <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	<span class="o">&amp;&amp;</span> pip install <span class="nv">torch</span><span class="o">==</span>2.2.2 <span class="nv">torchvision</span><span class="o">==</span>0.17.2 <span class="nv">torchaudio</span><span class="o">==</span>2.2.2 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	--index-url https://download.pytorch.org/whl/cu121 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	<span class="o">&amp;&amp;</span> pip install jupyterlab matplotlib pandas scikit-learn ipywidgets <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	<span class="o">&amp;&amp;</span> pip install transformers accelerate sentencepiece einops <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	<span class="o">&amp;&amp;</span> pip install langchain bitsandbytes protobuf <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	<span class="o">&amp;&amp;</span> pip install auto-gptq optimum <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	<span class="o">&amp;&amp;</span> pip install pypdf sentence-transformers <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	<span class="o">&amp;&amp;</span> pip install llama-index-embeddings-huggingface llama-index-llms-openai <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	<span class="o">&amp;&amp;</span> pip install llama-index-llms-llama-cpp <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	<span class="o">&amp;&amp;</span> pip install llama-index<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Install llama-cpp-python[server] with cuBLAS on</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> <span class="nv">CMAKE_ARGS</span><span class="o">=</span><span class="s2">&#34;-DLLAMA_CUBLAS=on&#34;</span> <span class="nv">FORCE_CMAKE</span><span class="o">=</span><span class="m">1</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        pip install llama-cpp-python<span class="o">[</span>server<span class="o">]</span> --force-reinstall --no-cache-dir<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Create a working directory</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">WORKDIR</span><span class="s"> /workdir</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Port number in container side</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">EXPOSE</span><span class="s"> 8888</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">ENTRYPOINT</span> <span class="p">[</span><span class="s2">&#34;jupyter-lab&#34;</span><span class="p">,</span> <span class="s2">&#34;--ip=0.0.0.0&#34;</span><span class="p">,</span> <span class="s2">&#34;--port=8888&#34;</span><span class="p">,</span> <span class="s2">&#34;--no-browser&#34;</span><span class="p">,</span> <span class="s2">&#34;--allow-root&#34;</span><span class="p">,</span> <span class="s2">&#34;--NotebookApp.token=&#39;&#39;&#34;</span><span class="p">]</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">CMD</span> <span class="p">[</span><span class="s2">&#34;--notebook-dir=/workdir&#34;</span><span class="p">]</span><span class="err">
</span></span></span></code></pre></div><p>An excerpt of the pip list for the container created in the above Dockerfile.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">llama_cpp_python                        0.2.75
</span></span><span class="line"><span class="cl">llama-index                             0.10.38
</span></span><span class="line"><span class="cl">llama-index-agent-openai                0.2.5
</span></span><span class="line"><span class="cl">llama-index-cli                         0.1.12
</span></span><span class="line"><span class="cl">llama-index-core                        0.10.38.post2
</span></span><span class="line"><span class="cl">llama-index-embeddings-huggingface      0.2.0
</span></span><span class="line"><span class="cl">llama-index-embeddings-openai           0.1.10
</span></span><span class="line"><span class="cl">llama-index-indices-managed-llama-cloud 0.1.6
</span></span><span class="line"><span class="cl">llama-index-legacy                      0.9.48
</span></span><span class="line"><span class="cl">llama-index-llms-llama-cpp              0.1.3
</span></span><span class="line"><span class="cl">llama-index-llms-openai                 0.1.20
</span></span><span class="line"><span class="cl">llama-index-multi-modal-llms-openai     0.1.6
</span></span><span class="line"><span class="cl">llama-index-program-openai              0.1.6
</span></span><span class="line"><span class="cl">llama-index-question-gen-openai         0.1.3
</span></span><span class="line"><span class="cl">llama-index-readers-file                0.1.22
</span></span><span class="line"><span class="cl">llama-index-readers-llama-parse         0.1.4
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">torch                                   2.2.2+cu121
</span></span><span class="line"><span class="cl">torchaudio                              2.2.2+cu121
</span></span><span class="line"><span class="cl">torchvision                             0.17.2+cu121
</span></span></code></pre></div><h4 id="python-source-changes">python source changes</h4>
<p>As mentioned in the Dockerfile section, the library configuration of llama_index has changed since v0.10.0, and the previous code would result in the following error.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">---------------------------------------------------------------------------</span>
</span></span><span class="line"><span class="cl"><span class="ne">ImportError</span>                               <span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl">      <span class="mi">2</span> <span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl">      <span class="mi">3</span> <span class="kn">import</span> <span class="nn">sys</span>
</span></span><span class="line"><span class="cl"><span class="o">----&gt;</span> <span class="mi">5</span> <span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="p">(</span>
</span></span><span class="line"><span class="cl">      <span class="mi">6</span>     <span class="n">LLMPredictor</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="mi">7</span>     <span class="n">PromptTemplate</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="mi">8</span>     <span class="n">ServiceContext</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="mi">9</span>     <span class="n">SimpleDirectoryReader</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="mi">10</span>     <span class="n">VectorStoreIndex</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">     <span class="mi">11</span> <span class="p">)</span>
</span></span><span class="line"><span class="cl">     <span class="mi">12</span> <span class="kn">from</span> <span class="nn">llama_index.callbacks</span> <span class="kn">import</span> <span class="n">CallbackManager</span><span class="p">,</span> <span class="n">LlamaDebugHandler</span>
</span></span><span class="line"><span class="cl">     <span class="mi">13</span> <span class="kn">from</span> <span class="nn">llama_index.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbedding</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="ne">ImportError</span><span class="p">:</span> <span class="n">cannot</span> <span class="kn">import</span> <span class="nn">name</span> <span class="s1">&#39;LLMPredictor&#39;</span> <span class="kn">from</span> <span class="s1">&#39;llama_index&#39;</span> <span class="p">(</span><span class="n">unknown</span> <span class="n">location</span><span class="p">)</span>
</span></span></code></pre></div><p>So, I modified it as follows.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">logging</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">os</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">sys</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">from llama_index import (
</span></span></span><span class="line"><span class="cl"><span class="s2">    LLMPredictor,
</span></span></span><span class="line"><span class="cl"><span class="s2">    PromptTemplate,
</span></span></span><span class="line"><span class="cl"><span class="s2">    ServiceContext,
</span></span></span><span class="line"><span class="cl"><span class="s2">    SimpleDirectoryReader,
</span></span></span><span class="line"><span class="cl"><span class="s2">    VectorStoreIndex,
</span></span></span><span class="line"><span class="cl"><span class="s2">)
</span></span></span><span class="line"><span class="cl"><span class="s2">
</span></span></span><span class="line"><span class="cl"><span class="s2">from llama_index.callbacks import CallbackManager, LlamaDebugHandler
</span></span></span><span class="line"><span class="cl"><span class="s2">from llama_index.embeddings import HuggingFaceEmbedding
</span></span></span><span class="line"><span class="cl"><span class="s2">from llama_index.llms import LlamaCPP
</span></span></span><span class="line"><span class="cl"><span class="s2">&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">llama_index.legacy</span> <span class="kn">import</span> <span class="n">LLMPredictor</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">llama_index.core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">llama_index.core</span> <span class="kn">import</span> <span class="n">ServiceContext</span><span class="p">,</span> <span class="n">SimpleDirectoryReader</span><span class="p">,</span> <span class="n">VectorStoreIndex</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">llama_index.core.callbacks</span> <span class="kn">import</span> <span class="n">CallbackManager</span><span class="p">,</span> <span class="n">LlamaDebugHandler</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">llama_index.embeddings.huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbedding</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">llama_index.llms.llama_cpp</span> <span class="kn">import</span> <span class="n">LlamaCPP</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># ログレベルの設定</span>
</span></span><span class="line"><span class="cl"><span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">,</span> <span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></div><p>The part of the f string specifying the LLM path is as follows for my environment.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">model_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&#34;../20240421_llamacpp/ELYZA-japanese-Llama-2-7b-fast-instruct-q4_K_M.gguf&#34;</span>
</span></span></code></pre></div><p>To use GPU, the n_gpu_layers should be</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">model_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;n_ctx&#34;</span><span class="p">:</span> <span class="mi">4096</span><span class="p">,</span> <span class="s2">&#34;n_gpu_layers&#34;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">},</span>
</span></span></code></pre></div><p>In addition, the EMBEDDING_DEVICE should be</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">EMBEDDING_DEVICE</span> <span class="o">=</span> <span class="s2">&#34;cuda&#34;</span>
</span></span></code></pre></div><p>In addition, in the explanation of the article, it was said that chunk_size was set to 512, so it was changed as follows.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
</span></span></code></pre></div><p>Other than that, I used the code from source 1. as is.</p>
<h4 id="how-it-works">How it works</h4>
<p>When I ran the code, I waited about 5 minutes for it to finish loading the LLM (Elyza model quantized by gguf), the RAG embedding model, and indexing.</p>
<p>When I threw a question, the answer came back in a couple of seconds.</p>
<p>GPU utilization is around 60-70% when returning answers. can run on both RTX A4000 (16GB) and TITAN V (12GB). used about 9GB of GPU memory.</p>
<h4 id="creating-an-embedded-model-from-a-web-page">Creating an embedded model from a web page</h4>
<p>In the source 1, I used text data for the embedded model; I further tried using data from Wikipedia and other web pages with reference to source 4.</p>
<p>The page <a href="https://ja.wikipedia.org/wiki/R%E9%81%8E%E7%A8%8B">r process</a> was used as the web page. The following cells were</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># ドキュメントの読み込み</span>
</span></span><span class="line"><span class="cl"><span class="n">documents</span> <span class="o">=</span> <span class="n">SimpleDirectoryReader</span><span class="p">(</span><span class="s2">&#34;data&#34;</span><span class="p">)</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
</span></span></code></pre></div><p>The following changes were made</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">target_url</span> <span class="o">=</span> <span class="s2">&#34;https://ja.wikipedia.org/wiki/R</span><span class="si">%E</span><span class="s2">9</span><span class="si">%81%</span><span class="s2">8E</span><span class="si">%E</span><span class="s2">7%A8%8B&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># NOTE: the html_to_text=True option requires html2text to be installed</span>
</span></span><span class="line"><span class="cl"><span class="n">documents</span> <span class="o">=</span> <span class="n">SimpleWebPageReader</span><span class="p">(</span><span class="n">html_to_text</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="n">target_url</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span></code></pre></div><p>The results of the execution are as follows (translating output results in Japanese to English)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">## Question: what is the mechanism of the r-process in neutron star mergers?</span>
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="n">omitted</span><span class="p">)</span><span class="o">.</span>
</span></span><span class="line"><span class="cl"><span class="c1">## Answer:</span>
</span></span><span class="line"><span class="cl"> <span class="n">The</span> <span class="n">r</span><span class="o">-</span><span class="n">process</span> <span class="n">can</span> <span class="n">occur</span> <span class="k">as</span> <span class="n">a</span> <span class="n">result</span> <span class="n">of</span> <span class="n">neutron</span> <span class="n">star</span> <span class="n">mergers</span><span class="o">.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Neutron</span> <span class="n">stars</span> <span class="n">are</span> <span class="n">very</span> <span class="n">dense</span><span class="p">,</span> <span class="ow">and</span> <span class="ow">in</span> <span class="n">them</span> <span class="n">nuclei</span> <span class="k">with</span> <span class="n">unstable</span> <span class="n">neutron</span> <span class="n">capture</span> <span class="n">are</span> <span class="n">produced</span><span class="o">.</span> <span class="n">This</span> <span class="n">produced</span> <span class="n">nucleus</span> <span class="k">with</span> <span class="n">unstable</span> <span class="n">neutron</span> <span class="n">capture</span> <span class="n">repeatedly</span> <span class="n">beta</span><span class="o">-</span><span class="n">decays</span> <span class="ow">and</span> <span class="n">incorporates</span> <span class="n">neutrons</span> <span class="n">into</span> <span class="n">nuclei</span> <span class="n">such</span> <span class="k">as</span> <span class="n">nickel56</span><span class="p">,</span> <span class="n">resulting</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">process</span> <span class="n">called</span> <span class="n">the</span> <span class="n">r</span><span class="o">-</span><span class="n">process</span><span class="o">.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">## Question: explain the co-evolution of galaxies and black holes</span>
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="n">omitted</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">## Answer:</span>
</span></span><span class="line"><span class="cl"> <span class="n">Information</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">context</span> <span class="n">information</span> <span class="n">will</span> <span class="ow">not</span> <span class="n">be</span> <span class="n">included</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">answer</span><span class="o">.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Also</span><span class="p">,</span> <span class="n">the</span> <span class="n">answer</span> <span class="n">to</span> <span class="n">this</span> <span class="n">question</span> <span class="ow">is</span> <span class="s2">&#34;I don&#39;t know&#34;</span> <span class="n">because</span> <span class="n">there</span> <span class="ow">is</span> <span class="n">no</span> <span class="n">information</span> <span class="n">that</span> <span class="n">the</span> <span class="n">release</span> <span class="n">of</span> <span class="n">energy</span> <span class="kn">from</span> <span class="nn">neutron</span> <span class="n">star</span> <span class="n">mergers</span> <span class="n">ultimately</span> <span class="n">results</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">black</span> <span class="n">hole</span><span class="o">.</span>
</span></span></code></pre></div><p>I was able to get a slightly more technical answer to the question about the r-process, but when I asked questions about content/concepts that are not in the embedded model, such as the question about the co-evolution of galaxies and black holes, I was not able to get an answer.</p>
<h2 id="future">Future</h2>
<p>I would like to challenge the following issues regarding data loading into embedding model.</p>
<ul>
<li>multiple websites and an entire website</li>
<li>PDFs</li>
<li>combinations of text files, PDFs, websites.</li>
</ul>
<p>I would like to deepen my understanding of the structure of llamaindex and improve the accuracy of my answers using RAG.</p>

        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240525_llamaindex_rag%2f&amp;text=Try%20RAG%20with%20LlamaIndex&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240525_llamaindex_rag%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240525_llamaindex_rag%2f&amp;title=Try%20RAG%20with%20LlamaIndex" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240525_llamaindex_rag%2f&amp;title=Try%20RAG%20with%20LlamaIndex" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240525_llamaindex_rag%2f&amp;title=Try%20RAG%20with%20LlamaIndex" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240525_llamaindex_rag%2f&amp;description=Try%20RAG%20with%20LlamaIndex" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://akenji3.github.io/en/post/20240506_llamaserver/" data-toggle="tooltip" data-placement="top" title="Try the Chatbot UI">&larr; Previous Post</a>
            </li>
          
          
            <li class="next">
              <a href="https://akenji3.github.io/en/post/20240704_numpy_v2/" data-toggle="tooltip" data-placement="top" title="llama-cpp-python - impact of numpy version upgrade">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      

    </div>
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          
          
        </ul>
        <p class="credits copyright text-muted">
          

          &nbsp;&bull;&nbsp;&copy;
          
            2025
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://akenji3.github.io/en/">akenji&#39;s lab</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.147.1</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="https://akenji3.github.io/js/main.js"></script>
<script src="https://akenji3.github.io/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://akenji3.github.io/js/load-photoswipe.js"></script>










    
  </body>
</html>

