

<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>running rinna 3.6b on a docker container - </title>

  <meta name="description" content="Motivation
I wanted to try out a large-scale language model (LLM) for Japanese, so I used rinna, which was released in May. To save installation time, I ran rinna under a docker container environment.
I ran into some problems in doing so, which are summarized below."><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "akenji\u0027s lab",
    
    "url": "https:\/\/akenji3.github.io\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/akenji3.github.io\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/akenji3.github.io\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/akenji3.github.io\/en\/post\/20230814_rinna36b\/",
          "name": "Running rinna 3.6b on a docker container"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : ""
  },
  "headline": "running rinna 3.6b on a docker container",
  "description" : "Motivation I wanted to try out a large-scale language model (LLM) for Japanese, so I used rinna, which was released in May. To save installation time, I ran rinna under a docker container environment.\nI ran into some problems in doing so, which are summarized below.\n",
  "inLanguage" : "en",
  "wordCount":  2305 ,
  "datePublished" : "2023-08-14T00:00:00\u002b00:00",
  "dateModified" : "2023-08-14T00:00:00\u002b00:00",
  "image" : "https:\/\/akenji3.github.io\/img\/avatar-icon.png",
  "keywords" : [ "" ],
  "mainEntityOfPage" : "https:\/\/akenji3.github.io\/en\/post\/20230814_rinna36b\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/akenji3.github.io\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/akenji3.github.io\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>


<meta property="og:title" content="running rinna 3.6b on a docker container" />
<meta property="og:description" content="Motivation
I wanted to try out a large-scale language model (LLM) for Japanese, so I used rinna, which was released in May. To save installation time, I ran rinna under a docker container environment.
I ran into some problems in doing so, which are summarized below.">
<meta property="og:image" content="https://akenji3.github.io/img/avatar-icon.png" />
<meta property="og:url" content="https://akenji3.github.io/en/post/20230814_rinna36b/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="akenji&#39;s lab" />

  <meta name="twitter:title" content="running rinna 3.6b on a docker container" />
  <meta name="twitter:description" content="Motivation
I wanted to try out a large-scale language model (LLM) for Japanese, so I used rinna, which was released in May. To save installation time, I ran rinna under a docker container environment. …">
  <meta name="twitter:image" content="https://akenji3.github.io/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='https://akenji3.github.io/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.147.1">
  <link rel="alternate" href="https://akenji3.github.io/en/index.xml" type="application/rss+xml" title="akenji&#39;s lab"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.6.0/css/all.css" integrity="sha384-h/hnnw1Bi4nbpD6kE7nYfCXzovi622sY5WBxww8ARKwpdLj5kUWjRuyiXaD1U2JT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="https://akenji3.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://akenji3.github.io/css/highlight.min.css" /><link rel="stylesheet" href="https://akenji3.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-TGXWYJXF48"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TGXWYJXF48');
        }
      </script>
  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://akenji3.github.io/en/">akenji&#39;s lab</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/en/">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="/en/page/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Categories" href="/en/categories">Categories</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/en/tags">Tags</a>
            </li>
          
        

        
          
            <li>
              
                
                  <a href="https://akenji3.github.io/post/20230814_rinna36b/">ja</a>
                
              
            </li>
          
        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="akenji&#39;s lab" href="https://akenji3.github.io/en/">
            <img class="avatar-img" src="https://akenji3.github.io/img/avatar-icon.png" alt="akenji&#39;s lab" />
           
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>running rinna 3.6b on a docker container</h1>
              
              
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;Posted on August 14, 2023
  
  
  
  
    
      &nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;
    
  
  &nbsp;&bull;&nbsp;Other languages: <a href="https://akenji3.github.io/post/20230814_rinna36b/" lang="ja">ja</a>
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <h1 id="motivation">Motivation</h1>
<p>I wanted to try out a large-scale language model (LLM) for Japanese, so I used rinna, which was released in May. To save installation time, I ran rinna under a docker container environment.</p>
<p>I ran into some problems in doing so, which are summarized below.</p>
<h2 id="sources">Sources.</h2>
<p>1.<a href="https://rinna.co.jp/news/2023/05/20230507.html">rinna press release</a> - See here about rinna. 3.6 billion parameters. rinna we tried this time is described as rinna 3.6b below. 2.
2.<a href="https://internet.watch.impress.co.jp/docs/column/shimizu/1503707.html">rinna introduction article</a> - An article from INTERNET Watch.　The sample code is also adapted from this article. 3.
3.<a href="https://qiita.com/Blaster36/items/246899ebe295d057b40b#fn-5">I created a story outline of the movie &ldquo;My Neighbor Totoro&rdquo; using the GPT Japanese language model (the first one)</a> - Container with Pytorch. The content regarding the images was based on this article.</p>
<h2 id="execution-result-of-rinna-36b">Execution result of rinna 3.6b</h2>
<h3 id="python-code-to-run">python code to run</h3>
<p>Save the sample code from source 2. as ${HOME}/workspace/rinna.py.</p>
<p>The contents of the sample code are as follows: 5th model uncommented and enabled.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">import <span class="nb">time</span>
</span></span><span class="line"><span class="cl">import torch
</span></span><span class="line"><span class="cl">from transformers import AutoTokenizer, AutoModelForCausalLM
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">tokenizer</span> <span class="o">=</span> AutoTokenizer.from_pretrained<span class="o">(</span><span class="s2">&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;</span>, <span class="nv">use_fast</span><span class="o">=</span>False<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 標準</span>
</span></span><span class="line"><span class="cl"><span class="c1">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 自動</span>
</span></span><span class="line"><span class="cl"><span class="c1">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, device_map=&#39;auto&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 自動(VRAM16GB以下でも8GBはNG)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, torch_dtype=torch.float16, device_map=&#39;auto&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># CPU指定</span>
</span></span><span class="line"><span class="cl"><span class="c1">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;).to(&#34;cpu&#34;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># GPU指定</span>
</span></span><span class="line"><span class="cl"><span class="nv">model</span> <span class="o">=</span> AutoModelForCausalLM.from_pretrained<span class="o">(</span><span class="s2">&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;</span><span class="o">)</span>.to<span class="o">(</span><span class="s2">&#34;cuda&#34;</span><span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># GPU指定(VRAM16GB以下でも8GBはNG)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, torch_dtype=torch.float16).to(&#34;cuda&#34;)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#繰り返し(抜けるのはCtl+c)</span>
</span></span><span class="line"><span class="cl"><span class="nv">prompt</span> <span class="o">=</span> <span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl"><span class="k">while</span> True:
</span></span><span class="line"><span class="cl">    <span class="c1"># 質問を入力</span>
</span></span><span class="line"><span class="cl">    <span class="nv">question</span> <span class="o">=</span> input<span class="o">(</span><span class="s2">&#34;質問をどうぞ： &#34;</span><span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> question.lower<span class="o">()</span> <span class="o">==</span> <span class="s1">&#39;clear&#39;</span>:
</span></span><span class="line"><span class="cl">        <span class="nv">question</span> <span class="o">=</span> <span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="nv">prompt</span> <span class="o">=</span> <span class="s2">&#34;&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">prompt</span> <span class="o">=</span> prompt+f<span class="s2">&#34;ユーザー: {question}&lt;NL&gt;システム: &#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 時間計測開始</span>
</span></span><span class="line"><span class="cl">    <span class="nv">start</span> <span class="o">=</span> time.time<span class="o">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">token_ids</span> <span class="o">=</span> tokenizer.encode<span class="o">(</span>prompt, <span class="nv">add_special_tokens</span><span class="o">=</span>False, <span class="nv">return_tensors</span><span class="o">=</span><span class="s2">&#34;pt&#34;</span><span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    with torch.no_grad<span class="o">()</span>:
</span></span><span class="line"><span class="cl">        <span class="nv">output_ids</span> <span class="o">=</span> model.generate<span class="o">(</span>
</span></span><span class="line"><span class="cl">            token_ids.to<span class="o">(</span>model.device<span class="o">)</span>,
</span></span><span class="line"><span class="cl">            <span class="nv">do_sample</span><span class="o">=</span>True,
</span></span><span class="line"><span class="cl">            <span class="nv">max_new_tokens</span><span class="o">=</span>128,
</span></span><span class="line"><span class="cl">            <span class="nv">temperature</span><span class="o">=</span>0.7,
</span></span><span class="line"><span class="cl">            <span class="nv">pad_token_id</span><span class="o">=</span>tokenizer.pad_token_id,
</span></span><span class="line"><span class="cl">            <span class="nv">bos_token_id</span><span class="o">=</span>tokenizer.bos_token_id,
</span></span><span class="line"><span class="cl">            <span class="nv">eos_token_id</span><span class="o">=</span>tokenizer.eos_token_id
</span></span><span class="line"><span class="cl">        <span class="o">)</span>
</span></span><span class="line"><span class="cl">    <span class="nv">output</span> <span class="o">=</span> tokenizer.decode<span class="o">(</span>output_ids.tolist<span class="o">()[</span>0<span class="o">][</span>token_ids.size<span class="o">(</span>1<span class="o">)</span>:<span class="o">])</span>
</span></span><span class="line"><span class="cl">    <span class="nv">output</span> <span class="o">=</span> output.replace<span class="o">(</span><span class="s2">&#34;&lt;NL&gt;&#34;</span>, <span class="s2">&#34;\n&#34;</span><span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 時間表示</span>
</span></span><span class="line"><span class="cl">    <span class="nv">end</span> <span class="o">=</span> time.time<span class="o">()</span>
</span></span><span class="line"><span class="cl">    print<span class="o">(</span>end-start<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#    print(prompt)</span>
</span></span><span class="line"><span class="cl">    print<span class="o">(</span>output<span class="o">)</span>
</span></span><span class="line"><span class="cl">    <span class="nv">prompt</span> <span class="o">=</span> prompt+output+<span class="s2">&#34;&lt;NL&gt;&#34;</span>
</span></span></code></pre></div><h3 id="download-from-ngc-catalog-and-run-the-container">Download from NGC CATALOG and run the container</h3>
<p>Referring to source 3., pull (download) <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/">Pytorch container</a> from <a href="https://catalog.ngc.nvidia.com/">NGC CATALOG</a>.</p>
<p>Look at the Tags on the <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch/tags">Catlog &gt; Containers &gt; Pytorch page</a> and see that 23.07-py3 is the latest, so download and run it immediately. For more information about docker running in user mode, please refer to <a href="https://akenji3.github.io/en/post/20230502_rootlessdocker_en/">this page</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ mkdir <span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/workspace
</span></span><span class="line"><span class="cl">$ docker pull nvcr.io/nvidia/pytorch:23.07-py3
</span></span><span class="line"><span class="cl">$ docker run -d -it --gpus all --name gpt -v <span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/workspace:/workspace/local nvcr.io/nvidia/pytorch:23.07-py3
</span></span><span class="line"><span class="cl">$ docker <span class="nb">exec</span> -it gpt /bin/bash
</span></span></code></pre></div><h3 id="running-on-rtx-a4000">Running on RTX A4000</h3>
<h4 id="operating-in-a-docker-environment">Operating in a docker environment</h4>
<p>From here, we will operate in the docker container launched above.</p>
<p>Lines starting with a # prompt indicate that the operation is performed in a container environment.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># python -m pip install transformers sentencepiece accelerate</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python rinna.py</span>
</span></span></code></pre></div><h4 id="deferredcudacallerror">DeferredCudaCallError</h4>
<p>The following error occurred while executing the above python code.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">raise DeferredCudaCallError<span class="o">(</span>msg<span class="o">)</span> from e
</span></span><span class="line"><span class="cl">torch.cuda.DeferredCudaCallError: CUDA call failed lazily at initialization with error: device &gt;<span class="o">=</span> <span class="m">0</span> <span class="o">&amp;&amp;</span> device &lt; num_gpus INTERNAL ASSERT FAILED at <span class="s2">&#34;/opt/pytorch/pytorch/aten/src/ATen/cuda/CUDAContext.\
</span></span></span><span class="line"><span class="cl"><span class="s2">cpp&#34;</span>:50, please report a bug to PyTorch. <span class="nv">device</span><span class="o">=</span>1, <span class="nv">num_gpus</span><span class="o">=</span>
</span></span></code></pre></div><p>I looked into it and found some comments on the internet that it was an error during MIG setup. My workstation is equipped with two GPUs, an A4000 and a Quadro K600. Given this, I figured I needed to specify which GPU to use, so I did the following</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># python -m pip install transformers sentencepiece accelerate</span>
</span></span><span class="line"><span class="cl"><span class="c1"># export CUDA_VISIBLE_DEVICES=&#34;0&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python rinna.py</span>
</span></span></code></pre></div><h4 id="execution-result">Execution Result</h4>
<p>The string following &ldquo;Ask a question(質問をどうぞ):&rdquo; was entered below.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">You are using the legacy behaviour of the &lt;class <span class="s1">&#39;transformers.models.t5.tokenization_t5.T5Tokenizer&#39;</span>&gt;. This means that tokens that come after special tokens will not be properly handled. We recommend you to <span class="nb">read</span> the related pull request available at https://github.com/huggingface/transformers/pull/24565
</span></span><span class="line"><span class="cl">質問をどうぞ： 日本の終戦記念日はいつですか
</span></span><span class="line"><span class="cl">1.9004747867584229
</span></span><span class="line"><span class="cl">1945年8月15日です。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 江戸幕府を開いたのは誰ですか
</span></span><span class="line"><span class="cl">1.9013473987579346
</span></span><span class="line"><span class="cl">徳川家康は1603年に亡くなり、1604年に将軍になりました。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 大化の改新について教えてください
</span></span><span class="line"><span class="cl">5.68342661857605
</span></span><span class="line"><span class="cl">大化の改新は、天智天皇が日本の政治改革を行い、中央集権政府を確立した革命的な政治改革でした。この改革<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>は中央政府を強化し、律令制度を確立しました。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： ありがとうございました
</span></span><span class="line"><span class="cl">0.6675028800964355
</span></span><span class="line"><span class="cl">どういたしまして&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ：
</span></span></code></pre></div><p>The Edo Shogunate&rsquo;s answer is a bit odd, but I&rsquo;ll ignore the details.</p>
<h3 id="running-on-titan-v">Running on TITAN V</h3>
<p>The same was executed on another workstation implementing TITAN V (+Quadro K2000).</p>
<p>As above, I downloaded the pytorch container from NGC CATALOG and executed rinna.py stored in ${HOME}/workspace. The operating procedure is as follows.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ docker run -d -it --gpus all --name gpt -v <span class="si">${</span><span class="nv">HOEM</span><span class="si">}</span>/workspace:/workspace/local <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>nvcr.io/nvidia/pytorch:23.07-py3
</span></span><span class="line"><span class="cl">$ docker <span class="nb">exec</span> -it gpt /bin/bash
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">### The following are operations in a container ###</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># export CUDA_VISIBLE_DEVICES=&#34;0&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python -m pip install transformers sentencepiece accelerate</span>
</span></span><span class="line"><span class="cl"><span class="c1"># cd local</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python rinna.py</span>
</span></span></code></pre></div><h4 id="nvidia-driver-is-too-old">NVIDIA driver is too old</h4>
<p>When I run the above, I get the following error</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">RuntimeError: The NVIDIA driver on your system is too old <span class="o">(</span>found version 11040<span class="o">)</span>. Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.
</span></span></code></pre></div><p>The same container that worked on the A4000 resulted in &ldquo;NVIDIA driver on your system is too old&rdquo; on TITAN V, as described above, and the reason is unknown.</p>
<p>Assuming that the NVIDIA driver is not the cause, but that pytorch does not support TITAN V, we decided to try with an older version of the pytorch container.</p>
<p>In case you are wondering, the torch-related version of the pytorch container in 23.07-py3 was as follows</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># pip list | grep torch</span>
</span></span><span class="line"><span class="cl">pytorch-quantization      2.1.2
</span></span><span class="line"><span class="cl">torch                     2.1.0a0+b5021ba
</span></span><span class="line"><span class="cl">torch-tensorrt            1.5.0.dev0
</span></span><span class="line"><span class="cl">torchdata                 0.7.0a0
</span></span><span class="line"><span class="cl">torchtext                 0.16.0a0
</span></span><span class="line"><span class="cl">torchvision               0.16.0a0
</span></span></code></pre></div><h4 id="trying-with-a-one-year-old-pytorch-container">Trying with a one year old pytorch container</h4>
<p>I decided to try with an older version of the pytorch container and, for no reason, I used the one year old 22.07-py3 as follows.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ docker run -d -it --gpus all --name gpt -v /home/kenji/workspace:/workspace/local <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>nvcr.io/nvidia/pytorch:22.07-py3
</span></span><span class="line"><span class="cl">$ docker <span class="nb">exec</span> -it gpt /bin/bash
</span></span></code></pre></div><p>From here, the operation is performed in a container. The version of torch is 1.13.0 as follows.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># pip list | grep torch</span>
</span></span><span class="line"><span class="cl">pytorch-quantization          2.1.2
</span></span><span class="line"><span class="cl">torch                         1.13.0a0+08820cb
</span></span><span class="line"><span class="cl">torch-tensorrt                1.2.0a0
</span></span><span class="line"><span class="cl">torchtext                     0.13.0a0
</span></span><span class="line"><span class="cl">torchvision                   0.14.0a0
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># export CUDA_VISIBLE_DEVICES=&#34;0&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python -m pip install transformers sentencepiece accelerate</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python rinna.py</span>
</span></span></code></pre></div><h4 id="cuda-out-of-memory">CUDA out of memory</h4>
<p>When the above was executed, the GPU ran out of VRAM capacity as shown below.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">RuntimeError: CUDA out of memory. Tried to allocate 122.00 MiB <span class="o">(</span>GPU 0<span class="p">;</span> 11.78 GiB total capacity<span class="p">;</span> 11.03 GiB already allocated<span class="p">;</span> 4.00 MiB free<span class="p">;</span> 11.12 GiB reserved in total by PyTorch<span class="o">)</span> If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation <span class="k">for</span> Memory Management and PYTORCH_CUDA_ALLOC_CONF
</span></span></code></pre></div><p>Since the 12GB of VRAM in TITAN V is not enough, I specify torch_dtype to reduce the model capacity.</p>
<p>Specifically, line 5 of model in rinna.py was commented out and line 6 was enabled.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># 標準</span>
</span></span><span class="line"><span class="cl"><span class="c1">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 自動</span>
</span></span><span class="line"><span class="cl"><span class="c1">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, device_map=&#39;auto&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 自動(VRAM16GB以下でも8GBはNG)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, torch_dtype=torch.float16, device_map=&#39;auto&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># CPU指定</span>
</span></span><span class="line"><span class="cl"><span class="c1">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;).to(&#34;cpu&#34;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># GPU指定</span>
</span></span><span class="line"><span class="cl"><span class="c1">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;).to(&#34;cuda&#34;)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># GPU指定(VRAM16GB以下でも8GBはNG)</span>
</span></span><span class="line"><span class="cl"><span class="nv">model</span> <span class="o">=</span> AutoModelForCausalLM.from_pretrained<span class="o">(</span><span class="s2">&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;</span>, <span class="nv">torch_dtype</span><span class="o">=</span>torch.float16<span class="o">)</span>.to<span class="o">(</span><span class="s2">&#34;cuda&#34;</span><span class="o">)</span>
</span></span></code></pre></div><h4 id="execution-result-1">Execution Result</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># python rinna.py</span>
</span></span><span class="line"><span class="cl">You are using the legacy behaviour of the &lt;class <span class="s1">&#39;transformers.models.t5.tokenization_t5.T5Tokenizer&#39;</span>&gt;. This means that tokens that come after special tokens will not be properly handled. We recommend you to <span class="nb">read</span> the related pull request available at https://github.com/huggingface/transformers/pull/24565
</span></span><span class="line"><span class="cl">質問をどうぞ： 日本の終戦記念日はいつですか
</span></span><span class="line"><span class="cl">1.9047455787658691
</span></span><span class="line"><span class="cl">1945年8月15日です。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 江戸幕府を開いたのは誰ですか
</span></span><span class="line"><span class="cl">0.19092082977294922
</span></span><span class="line"><span class="cl">徳川家康&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 大化の改新について教えてください
</span></span><span class="line"><span class="cl">4.939814329147339
</span></span><span class="line"><span class="cl">大化の改新とは、中大兄皇子、後の天智天皇が政治改革を行った一連の改革です。これらの改革は、公正な競争<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>のルールや、身分制度にとらわれない人々の平等な権利の確立など、その後の日本の政治システムの基盤を提供<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>しました。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 南北朝時代になった原因を教えてください
</span></span><span class="line"><span class="cl">1.415543794631958
</span></span><span class="line"><span class="cl">歴史の授業で、南北朝時代がどのように起こったかを説明しているのですか?&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 南北朝時代がどうして起こったのか教えてください
</span></span><span class="line"><span class="cl">9.151806592941284
</span></span><span class="line"><span class="cl">それは複雑な過程を経ています。一般的には、中国が北朝と南朝に分裂したと言えますが、それだけではありま<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>せん。北朝は、13世紀後半に始まった第1次モンゴル軍の侵攻や、14世紀後半の明朝の拡大に対処しなければな<span class="se">\\</span>
</span></span><span class="line"><span class="cl">らなかったため、大きく揺れ動いています。一方、南朝は、14世紀半ばの明朝による中国再統一の結果、安定し<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>た状態にありました。しかしながら、清盛が平氏政権を樹立した後、北朝は弱体化し、最終的には明朝による統<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>一によって終了することになりました。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ：
</span></span></code></pre></div><h2 id="summary">Summary</h2>
<p>This time, since I was running in a docker container, I thought it would be easy to run without any installation-related problems, but as mentioned above, I got into a bit of a bind. However, as mentioned above, I got into a bit of trouble. Thanks to the trouble, I was able to absorb a little bit of docker operation and related knowledge.</p>
<p>The part where I got stuck is as follows. The cause is based on my assumption and may be different from the actual situation.</p>
<ul>
<li>DeferredCudaCallError
This can occur when multiple GPUs are implemented, and it was solved by specifying which GPU is used in CUDA_VISIBLE_DEVICES.</li>
<li>The NVIDIA driver on your system is too old
The pytorch container you are using is too new and does not support your GPU architecture. Use the old pytorch container.
If you know the pytorch version and the supported sm (NVIDIA architecuter name), please let me know.</li>
<li>CUDA out of memory
Common GPU VRAM shortage. Reduce model size. In this case, I specified torch_dtype=torch.float16.</li>
</ul>

        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20230814_rinna36b%2f&amp;text=running%20rinna%203.6b%20on%20a%20docker%20container&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20230814_rinna36b%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20230814_rinna36b%2f&amp;title=running%20rinna%203.6b%20on%20a%20docker%20container" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20230814_rinna36b%2f&amp;title=running%20rinna%203.6b%20on%20a%20docker%20container" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20230814_rinna36b%2f&amp;title=running%20rinna%203.6b%20on%20a%20docker%20container" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20230814_rinna36b%2f&amp;description=running%20rinna%203.6b%20on%20a%20docker%20container" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://akenji3.github.io/en/post/20230709_ubuntu2024_install/" data-toggle="tooltip" data-placement="top" title="Install ubuntu 22.04 LTS">&larr; Previous Post</a>
            </li>
          
          
            <li class="next">
              <a href="https://akenji3.github.io/en/post/20230918_modulus_install/" data-toggle="tooltip" data-placement="top" title="Trying NVIDIA Modulus - Introduction to PINNs">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      

    </div>
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          
          
        </ul>
        <p class="credits copyright text-muted">
          

          &nbsp;&bull;&nbsp;&copy;
          
            2025
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://akenji3.github.io/en/">akenji&#39;s lab</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.147.1</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="https://akenji3.github.io/js/main.js"></script>
<script src="https://akenji3.github.io/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://akenji3.github.io/js/load-photoswipe.js"></script>










    
  </body>
</html>

