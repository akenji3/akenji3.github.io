

<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>3D Magnetohydrodynamic Simulation and Parallel Computing - Athena&#43;&#43; Tutorial 4 - </title>

  <meta name="description" content="Introduction
Up to this article posted last month, I have confirmed that OpenMPI can be embedded in a Docker container and used for parallel computing on multiple nodes. In this post, I will use the Docker container created above to run tutorial 4 &ldquo;Running 3D MHD with OpenMP and MPI&rdquo; of Athena&#43;&#43; on multiple nodes."><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "akenji\u0027s lab",
    
    "url": "https:\/\/akenji3.github.io\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/akenji3.github.io\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/akenji3.github.io\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/akenji3.github.io\/en\/post\/20240307_athena_tutorial4\/",
          "name": "3 d magnetohydrodynamic simulation and parallel computing athena tutorial 4"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : ""
  },
  "headline": "3D Magnetohydrodynamic Simulation and Parallel Computing - Athena\u002b\u002b Tutorial 4",
  "description" : "Introduction Up to this article posted last month, I have confirmed that OpenMPI can be embedded in a Docker container and used for parallel computing on multiple nodes. In this post, I will use the Docker container created above to run tutorial 4 \u0026ldquo;Running 3D MHD with OpenMP and MPI\u0026rdquo; of Athena\u002b\u002b on multiple nodes.\n",
  "inLanguage" : "en",
  "wordCount":  1260 ,
  "datePublished" : "2024-03-07T00:00:00\u002b00:00",
  "dateModified" : "2024-03-07T00:00:00\u002b00:00",
  "image" : "https:\/\/akenji3.github.io\/img\/avatar-icon.png",
  "keywords" : [ "" ],
  "mainEntityOfPage" : "https:\/\/akenji3.github.io\/en\/post\/20240307_athena_tutorial4\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/akenji3.github.io\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/akenji3.github.io\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>


<meta property="og:title" content="3D Magnetohydrodynamic Simulation and Parallel Computing - Athena&#43;&#43; Tutorial 4" />
<meta property="og:description" content="Introduction
Up to this article posted last month, I have confirmed that OpenMPI can be embedded in a Docker container and used for parallel computing on multiple nodes. In this post, I will use the Docker container created above to run tutorial 4 &ldquo;Running 3D MHD with OpenMP and MPI&rdquo; of Athena&#43;&#43; on multiple nodes.">
<meta property="og:image" content="https://akenji3.github.io/img/avatar-icon.png" />
<meta property="og:url" content="https://akenji3.github.io/en/post/20240307_athena_tutorial4/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="akenji&#39;s lab" />

  <meta name="twitter:title" content="3D Magnetohydrodynamic Simulation and Parallel Computing - Athena&#43;&#43; …" />
  <meta name="twitter:description" content="Introduction
Up to this article posted last month, I have confirmed that OpenMPI can be embedded in a Docker container and used for parallel computing on multiple nodes. In this post, I will use the …">
  <meta name="twitter:image" content="https://akenji3.github.io/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='https://akenji3.github.io/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.147.1">
  <link rel="alternate" href="https://akenji3.github.io/en/index.xml" type="application/rss+xml" title="akenji&#39;s lab"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.6.0/css/all.css" integrity="sha384-h/hnnw1Bi4nbpD6kE7nYfCXzovi622sY5WBxww8ARKwpdLj5kUWjRuyiXaD1U2JT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="https://akenji3.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://akenji3.github.io/css/highlight.min.css" /><link rel="stylesheet" href="https://akenji3.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-TGXWYJXF48"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TGXWYJXF48');
        }
      </script>
  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://akenji3.github.io/en/">akenji&#39;s lab</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/en/">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="/en/page/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Categories" href="/en/categories">Categories</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/en/tags">Tags</a>
            </li>
          
        

        
          
            <li>
              
                
                  <a href="https://akenji3.github.io/post/20240307_athena_tutorial4/">ja</a>
                
              
            </li>
          
        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="akenji&#39;s lab" href="https://akenji3.github.io/en/">
            <img class="avatar-img" src="https://akenji3.github.io/img/avatar-icon.png" alt="akenji&#39;s lab" />
           
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>3D Magnetohydrodynamic Simulation and Parallel Computing - Athena&#43;&#43; Tutorial 4</h1>
              
              
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;Posted on March 7, 2024
  
  
  
  
    
      &nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;
    
  
  &nbsp;&bull;&nbsp;Other languages: <a href="https://akenji3.github.io/post/20240307_athena_tutorial4/" lang="ja">ja</a>
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <h2 id="introduction">Introduction</h2>
<p>Up to <a href="https://akenji3.github.io/en/post/20240223_himenobenchmpi/">this article</a> posted last month, I have confirmed that OpenMPI can be embedded in a Docker container and used for parallel computing on multiple nodes. In this post, I will use the Docker container created above to run tutorial 4 &ldquo;Running 3D MHD with OpenMP and MPI&rdquo; of Athena++ on multiple nodes.</p>
<h2 id="sources">Sources.</h2>
<ol>
<li><a href="https://github.com/PrincetonUniversity/athena/wiki/Tutorial">Athena Tutorial</a>  This time, I will run &ldquo;4. Running 3D MHD with OpenMP and MPI&rdquo;.</li>
<li><a href="https://www.astr.tohoku.ac.jp/~tomida/athena/tutorial.html">Japanese page of the above</a>  This page is maintained by Dr. Kengo Tomida.</li>
<li><a href="https://github.com/anandogc/h5si/issues/2">H5Pset_fapl_mpio was not declared in this scope</a>  A page I searched to investigate the cause of the compile error and found that the parallel HDF5 library is required.</li>
<li><a href="https://github.com/geospace-code/h5fortran-mpi">h5fortran-mpi</a>  A page that I found by searching for &ldquo;parallel HDF5 library&rdquo;.  In ubuntu, that is libhdf5-mpi-dev package.</li>
</ol>
<h2 id="execution-environment">Execution Environment</h2>
<h4 id="dockerfile">Dockerfile</h4>
<p>The Dockerfile to run this tutorial is as follows</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-dockerfile" data-lang="dockerfile"><span class="line"><span class="cl"><span class="c"># Based on the Dockefile for creating a Docker image that JupyterLab can use,</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Create a Docker container that can be used for athena++ development and execution.</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Based on the latest version of ubuntu 22.04.</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">FROM</span><span class="s"> ubuntu:jammy-20240111</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Set bash as the default shell</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">ENV</span> <span class="nv">SHELL</span><span class="o">=</span>/bin/bash<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Build with some basic utilities</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> apt update <span class="o">&amp;&amp;</span> apt install -y <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    build-essential <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    python3-pip apt-utils vim <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    git git-lfs <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    curl unzip wget gnuplot <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    openmpi-bin libopenmpi-dev <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    openssh-client openssh-server <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    libhdf5-dev libhdf5-openmpi-dev<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># alias python=&#39;python3&#39;</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> ln -s /usr/bin/python3 /usr/bin/python<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># install python package to need</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> pip install -U pip setuptools <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>	<span class="o">&amp;&amp;</span> pip install numpy scipy h5py mpmath<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># The following stuff is derived for horovod in docker.</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Allow OpenSSH to talk to containers without asking for confirmation</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> mkdir -p /var/run/sshd<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">RUN</span> cat /etc/ssh/ssh_config <span class="p">|</span> grep -v StrictHostKeyChecking &gt; /etc/ssh/ssh_config.new <span class="o">&amp;&amp;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    <span class="nb">echo</span> <span class="s2">&#34;    StrictHostKeyChecking no&#34;</span> &gt;&gt; /etc/ssh/ssh_config.new <span class="o">&amp;&amp;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>    mv /etc/ssh/ssh_config.new /etc/ssh/ssh_config<span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># --allow-run-as-root</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">ENV</span> <span class="nv">OMPI_ALLOW_RUN_AS_ROOT</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl"><span class="k">ENV</span> <span class="nv">OMPI_ALLOW_RUN_AS_ROOT_CONFIRM</span><span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c"># Set hdf5 path</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">ENV</span> <span class="nv">CPATH</span><span class="o">=</span><span class="s2">&#34;/usr/include/hdf5/openmpi/&#34;</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># Create a working directory</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">WORKDIR</span><span class="s"> /workdir</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="c"># command prompt</span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="k">CMD</span> <span class="p">[</span><span class="s2">&#34;/bin/bash&#34;</span><span class="p">]</span><span class="err">
</span></span></span></code></pre></div><p>In the end, using the container created from the above Dockerfile, I was able to run this tutorial. The trial and error process to obtain this Dockerfile is described below. The point was to be able to use HDF5 in an MPI environment.</p>
<h4 id="a-record-of-the-errors-i-encountered-before-creating-a-container-to-use-hdf5">A record of the errors I encountered before creating a container to use HDF5.</h4>
<p>The first step is to make the following configuration.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># python configure.py --prob blast -b --flux hlld -mpi -hdf5</span>
</span></span><span class="line"><span class="cl">  Your Athena++ distribution has now been configured with the following options: 
</span></span><span class="line"><span class="cl">  Problem generator:            blast
</span></span><span class="line"><span class="cl">  Coordinate system:            cartesian
</span></span><span class="line"><span class="cl">  Equation of state:            adiabatic
</span></span><span class="line"><span class="cl">  Riemann solver:               hlld
</span></span><span class="line"><span class="cl">  Magnetic fields:              ON
</span></span><span class="line"><span class="cl">  Number of scalars:            <span class="m">0</span>
</span></span><span class="line"><span class="cl">  Number of chemical species:   <span class="m">0</span>
</span></span><span class="line"><span class="cl">  Special relativity:           OFF
</span></span><span class="line"><span class="cl">  General relativity:           OFF
</span></span><span class="line"><span class="cl">  Radiative Transfer:           OFF
</span></span><span class="line"><span class="cl">  Implicit Radiation:           OFF
</span></span><span class="line"><span class="cl">  Cosmic Ray Transport:         OFF
</span></span><span class="line"><span class="cl">  Frame transformations:        OFF
</span></span><span class="line"><span class="cl">  Self-Gravity:                 OFF
</span></span><span class="line"><span class="cl">  Super-Time-Stepping:          OFF
</span></span><span class="line"><span class="cl">  Chemistry:                    OFF
</span></span><span class="line"><span class="cl">  KIDA rates:                   OFF
</span></span><span class="line"><span class="cl">  ChemRadiation:                OFF
</span></span><span class="line"><span class="cl">  chem_ode_solver:              OFF
</span></span><span class="line"><span class="cl">  Debug flags:                  OFF
</span></span><span class="line"><span class="cl">  Code coverage flags:          OFF
</span></span><span class="line"><span class="cl">  Linker flags:                   -lhdf5
</span></span><span class="line"><span class="cl">  Floating-point precision:     double
</span></span><span class="line"><span class="cl">  Number of ghost cells:        <span class="m">2</span>
</span></span><span class="line"><span class="cl">  MPI parallelism:              ON
</span></span><span class="line"><span class="cl">  OpenMP parallelism:           OFF
</span></span><span class="line"><span class="cl">  FFT:                          OFF
</span></span><span class="line"><span class="cl">  HDF5 output:                  ON
</span></span><span class="line"><span class="cl">  HDF5 precision:               single
</span></span><span class="line"><span class="cl">  Compiler:                     g++
</span></span><span class="line"><span class="cl">  Compilation command:          mpicxx  -O3 -std<span class="o">=</span>c++11
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># make clean</span>
</span></span><span class="line"><span class="cl">rm -rf obj/*
</span></span><span class="line"><span class="cl">rm -rf bin/athena
</span></span><span class="line"><span class="cl">rm -rf *.gcov
</span></span></code></pre></div><h5 id="hdf5h-no-such-file-or-directory">hdf5.h: No such file or directory</h5>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># make</span>
</span></span><span class="line"><span class="cl">mpicxx  -O3 -std<span class="o">=</span>c++11 -c src/globals.cpp -o obj/globals.o
</span></span><span class="line"><span class="cl">mpicxx  -O3 -std<span class="o">=</span>c++11 -c src/main.cpp -o obj/main.o
</span></span><span class="line"><span class="cl">In file included from src/main.cpp:46:
</span></span><span class="line"><span class="cl">src/outputs/outputs.hpp:22:10: fatal error: hdf5.h: No such file or directory
</span></span><span class="line"><span class="cl">   <span class="m">22</span> <span class="p">|</span> <span class="c1">#include &lt;hdf5.h&gt;</span>
</span></span><span class="line"><span class="cl">      <span class="p">|</span>          ^~~~~~~~
</span></span><span class="line"><span class="cl">compilation terminated.
</span></span><span class="line"><span class="cl">make: *** <span class="o">[</span>Makefile:119: obj/main.o<span class="o">]</span> Error <span class="m">1</span>
</span></span></code></pre></div><p>To address the above error, I added libhdf5-dev to the Dockerfile to install. However, this alone did not solve the error, and export CPATH=&quot;/usr/include/hdf5/serial/&quot; was added to the Dockerfile.</p>
<p>After that, I made again.</p>
<h5 id="ifdef-mpi_parallel">#ifdef MPI_PARALLEL</h5>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># make</span>
</span></span><span class="line"><span class="cl">mpicxx  -O3 -std<span class="o">=</span>c++11 -c src/globals.cpp -o obj/globals.o
</span></span><span class="line"><span class="cl">・・・
</span></span><span class="line"><span class="cl">・・・
</span></span><span class="line"><span class="cl">mpicxx  -O3 -std<span class="o">=</span>c++11 -c src/inputs/hdf5_reader.cpp -o obj/hdf5_reader.o
</span></span><span class="line"><span class="cl">src/inputs/hdf5_reader.cpp: In <span class="k">function</span> <span class="s1">&#39;void HDF5ReadRealArray(const char*, const char*, int, const int*, const int*, int, const int*, const int*, AthenaArray&lt;double&gt;&amp;, bool, bool)&#39;</span>:
</span></span><span class="line"><span class="cl">src/inputs/hdf5_reader.cpp:94:7: error: <span class="s1">&#39;H5Pset_fapl_mpio&#39;</span> was not declared in this scope<span class="p">;</span> did you mean <span class="s1">&#39;H5Pset_fapl_stdio&#39;</span>?
</span></span><span class="line"><span class="cl">   <span class="m">94</span> <span class="p">|</span>       H5Pset_fapl_mpio<span class="o">(</span>property_list_file, MPI_COMM_WORLD, MPI_INFO_NULL<span class="o">)</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="p">|</span>       ^~~~~~~~~~~~~~~~
</span></span><span class="line"><span class="cl">      <span class="p">|</span>       H5Pset_fapl_stdio
</span></span><span class="line"><span class="cl">src/inputs/hdf5_reader.cpp:109:7: error: <span class="s1">&#39;H5Pset_dxpl_mpio&#39;</span> was not declared in this scope<span class="p">;</span> did you mean <span class="s1">&#39;H5Pset_fapl_stdio&#39;</span>?
</span></span><span class="line"><span class="cl">  <span class="m">109</span> <span class="p">|</span>       H5Pset_dxpl_mpio<span class="o">(</span>property_list_transfer, H5FD_MPIO_COLLECTIVE<span class="o">)</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">      <span class="p">|</span>       ^~~~~~~~~~~~~~~~
</span></span><span class="line"><span class="cl">      <span class="p">|</span>       H5Pset_fapl_stdio
</span></span><span class="line"><span class="cl">make: *** <span class="o">[</span>Makefile:119: obj/hdf5_reader.o<span class="o">]</span> Error <span class="m">1</span>
</span></span></code></pre></div><p>Since the above error was in the #ifdef MPI_PARALLEL section of the source code, I investigated whether hdf5-related modules might be required for parallel computing with OpenMPI.</p>
<p>I found the information in the sources 3 and 4, and installed libhdf5-openmpi-dev in the Dockerfile. I also changed the CPATH setting I added above to CPATH=&quot;/usr/include/hdf5/openmpi&quot;.</p>
<h5 id="link-error">link error</h5>
<p>When I tried to make again, I got the following error at the end of compiling and linking.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">/usr/bin/ld: cannot find -lhdf5: No such file or directory
</span></span><span class="line"><span class="cl">collect2: error: ld returned <span class="m">1</span> <span class="nb">exit</span> status
</span></span><span class="line"><span class="cl">make: *** <span class="o">[</span>Makefile:114: bin/athena<span class="o">]</span> Error <span class="m">1</span>
</span></span></code></pre></div><p>After checking under /usr/lib/x86_64-linux-gnu, where the library is stored, I guessed that the library I needed was hdf5_openmpi, and changed &ldquo;-lhdf5&rdquo; in the Makefile generated as a result of the configuration to &ldquo;-lhdf5_openmpi&rdquo;.</p>
<p>After above trial and error, I obtained the Dockerfile shown at the beginning of this section.</p>
<h2 id="running-the-simulation">Running the simulation</h2>
<h4 id="edit-parameter-files">Edit parameter files</h4>
<p>As in the previous tutorial, copy the parameter file (input file) and executable to the working directory &ldquo;t4&rdquo; in the container. The parameter file was copied from athena/inputs/mhd/athinput.blast.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># pwd</span>
</span></span><span class="line"><span class="cl">/workdir/kenji/t4
</span></span><span class="line"><span class="cl"><span class="c1"># ls -l</span>
</span></span><span class="line"><span class="cl">-rwxr-xr-x <span class="m">1</span> root root  <span class="m">3613256</span> Mar  <span class="m">2</span> 04:11 athena
</span></span><span class="line"><span class="cl">-rw-r--r-- <span class="m">1</span> root root     <span class="m">2193</span> Mar  <span class="m">2</span> 08:12 athinput.blast
</span></span></code></pre></div><p>This time, since the time is measured in parallel processing, I decided to set the mesh to double so that it would take a little longer. The changes to athinput.blast are as follows</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">10c10
</span></span><span class="line"><span class="cl">&lt; <span class="nv">file_type</span>  <span class="o">=</span> hdf5       <span class="c1"># HDF5 data dump</span>
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">&gt; <span class="nv">file_type</span>  <span class="o">=</span> vtk        <span class="c1"># VTK data dump</span>
</span></span><span class="line"><span class="cl">13d12
</span></span><span class="line"><span class="cl">&lt; <span class="nv">ghost_zones</span> <span class="o">=</span> <span class="nb">true</span>      <span class="c1"># enables ghost zone output</span>
</span></span><span class="line"><span class="cl">24c23
</span></span><span class="line"><span class="cl">&lt; <span class="nv">nx1</span>        <span class="o">=</span> <span class="m">128</span>         <span class="c1"># Number of zones in X1-direction</span>
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">&gt; <span class="nv">nx1</span>        <span class="o">=</span> <span class="m">64</span>         <span class="c1"># Number of zones in X1-direction</span>
</span></span><span class="line"><span class="cl">30c29
</span></span><span class="line"><span class="cl">&lt; <span class="nv">nx2</span>        <span class="o">=</span> <span class="m">128</span>         <span class="c1"># Number of zones in X2-direction</span>
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">&gt; <span class="nv">nx2</span>        <span class="o">=</span> <span class="m">64</span>         <span class="c1"># Number of zones in X2-direction</span>
</span></span><span class="line"><span class="cl">36c35
</span></span><span class="line"><span class="cl">&lt; <span class="nv">nx3</span>        <span class="o">=</span> <span class="m">128</span>         <span class="c1"># Number of zones in X3-direction</span>
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">&gt; <span class="nv">nx3</span>        <span class="o">=</span> <span class="m">64</span>         <span class="c1"># Number of zones in X3-direction</span>
</span></span><span class="line"><span class="cl">42,47c41
</span></span><span class="line"><span class="cl">&lt; <span class="c1">#num_threads = 1         # Number of OpenMP threads per process</span>
</span></span><span class="line"><span class="cl">&lt; 
</span></span><span class="line"><span class="cl">&lt; &lt;meshblock&gt;
</span></span><span class="line"><span class="cl">&lt; <span class="nv">nx1</span>        <span class="o">=</span> <span class="m">32</span>         <span class="c1"># Number of zones per MeshBlock in X1-direction</span>
</span></span><span class="line"><span class="cl">&lt; <span class="nv">nx2</span>        <span class="o">=</span> <span class="m">32</span>         <span class="c1"># Number of zones per MeshBlock in X2-direction</span>
</span></span><span class="line"><span class="cl">&lt; <span class="nv">nx3</span>        <span class="o">=</span> <span class="m">32</span>         <span class="c1"># Number of zones per MeshBlock in X3-direction</span>
</span></span><span class="line"><span class="cl">---
</span></span><span class="line"><span class="cl">&gt; <span class="nv">num_threads</span> <span class="o">=</span> <span class="m">1</span>         <span class="c1"># Number of OpenMP threads per process</span>
</span></span></code></pre></div><h4 id="measuring-simulation-time">Measuring simulation time</h4>
<p>Create the following shell command, run 2, 4, and 8 for the number of processes (np), and compare the cpu time recorded in log files.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># cat mpi_run</span>
</span></span><span class="line"><span class="cl">mpirun -np <span class="nv">$1</span> --hostfile myhosts <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>-mca plm_rsh_args <span class="s2">&#34;-p 12345&#34;</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>-mca btl_tcp_if_exclude lo,docker0 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>-oversubscribe <span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/athena <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>-i <span class="nv">$2</span> &gt; log
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># cat myhosts</span>
</span></span><span class="line"><span class="cl">europe <span class="nv">slots</span><span class="o">=</span><span class="m">4</span>
</span></span><span class="line"><span class="cl">jupiter <span class="nv">slots</span><span class="o">=</span><span class="m">4</span>
</span></span><span class="line"><span class="cl">ganymede <span class="nv">slots</span><span class="o">=</span><span class="m">6</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># mpi_run 2 athinput.blast</span>
</span></span><span class="line"><span class="cl"><span class="c1"># mpi_run 4 athinput.blast</span>
</span></span><span class="line"><span class="cl"><span class="c1"># mpi_run 8 athinput.blast</span>
</span></span></code></pre></div><h4 id="measurement-results">Measurement results</h4>
<p>The cpu time recorded in the log was as follows.</p>
<table>
  <thead>
      <tr>
          <th>number of processes (np)</th>
          <th>cpu time (sec)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>2</td>
          <td>878</td>
      </tr>
      <tr>
          <td>4</td>
          <td>520</td>
      </tr>
      <tr>
          <td>8</td>
          <td>408</td>
      </tr>
  </tbody>
</table>
<p>The graph is as follows.</p>
<p><img src="/images/20240307_Athena_Tutorial4/20240306_CPU_Time.png" alt="CPU_Time"></p>
<h2 id="for-the-future">For the future</h2>
<p>The measurement results showed that when 8 processes are processed simultaneously, the simulation can be run in about half the time of 2 processes.</p>
<p>I will continue to work on the visualization part of the simulation results for this tutorial.</p>
<p>In addition, Dr. Tomida&rsquo;s page in the source 2. has an additional task to try the simulation of &ldquo;Rayleigh-Taylor instability&rdquo;, which I would like to run.</p>

        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240307_athena_tutorial4%2f&amp;text=3D%20Magnetohydrodynamic%20Simulation%20and%20Parallel%20Computing%20-%20Athena%2b%2b%20Tutorial%204&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240307_athena_tutorial4%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240307_athena_tutorial4%2f&amp;title=3D%20Magnetohydrodynamic%20Simulation%20and%20Parallel%20Computing%20-%20Athena%2b%2b%20Tutorial%204" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240307_athena_tutorial4%2f&amp;title=3D%20Magnetohydrodynamic%20Simulation%20and%20Parallel%20Computing%20-%20Athena%2b%2b%20Tutorial%204" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240307_athena_tutorial4%2f&amp;title=3D%20Magnetohydrodynamic%20Simulation%20and%20Parallel%20Computing%20-%20Athena%2b%2b%20Tutorial%204" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fakenji3.github.io%2fen%2fpost%2f20240307_athena_tutorial4%2f&amp;description=3D%20Magnetohydrodynamic%20Simulation%20and%20Parallel%20Computing%20-%20Athena%2b%2b%20Tutorial%204" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://akenji3.github.io/en/post/20240223_himenobenchmpi/" data-toggle="tooltip" data-placement="top" title="easuring OpenMPI performance using the HIMENO benchmark">&larr; Previous Post</a>
            </li>
          
          
            <li class="next">
              <a href="https://akenji3.github.io/en/post/20240310_visit_tutorial4/" data-toggle="tooltip" data-placement="top" title="Visualization of Simulation Results - Athena&#43;&#43; Tutorial 4">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      

    </div>
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          
          
        </ul>
        <p class="credits copyright text-muted">
          

          &nbsp;&bull;&nbsp;&copy;
          
            2025
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://akenji3.github.io/en/">akenji&#39;s lab</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.147.1</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="https://akenji3.github.io/js/main.js"></script>
<script src="https://akenji3.github.io/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://akenji3.github.io/js/load-photoswipe.js"></script>










    
  </body>
</html>

