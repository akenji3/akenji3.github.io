

<!DOCTYPE html>
<html lang="ja" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>rinna 3.6bをdockerから使う - </title>

  <meta name="description" content="モチベーション
日本語に大規模言語モデル（LLM）を試してみたくて、5月にリリースされたrinnaを使ってみた。インストールの手間を省くため、dockerコンテナ環境下でrinnaを実行した。
その際にハマったこともあったので以下にまとめる。"><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "akenji\u0027s lab",
    
    "url": "https:\/\/akenji3.github.io\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/akenji3.github.io\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/akenji3.github.io\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/akenji3.github.io\/post\/20230814_rinna36b\/",
          "name": "Rinna 3.6bをdockerから使う"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : ""
  },
  "headline": "rinna 3.6bをdockerから使う",
  "description" : "モチベーション 日本語に大規模言語モデル（LLM）を試してみたくて、5月にリリースされたrinnaを使ってみた。インストールの手間を省くため、dockerコンテナ環境下でrinnaを実行した。\nその際にハマったこともあったので以下にまとめる。\n",
  "inLanguage" : "ja",
  "wordCount":  3460 ,
  "datePublished" : "2023-08-14T00:00:00\u002b00:00",
  "dateModified" : "2023-08-14T00:00:00\u002b00:00",
  "image" : "https:\/\/akenji3.github.io\/img\/avatar-icon.png",
  "keywords" : [ "" ],
  "mainEntityOfPage" : "https:\/\/akenji3.github.io\/post\/20230814_rinna36b\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/akenji3.github.io\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/akenji3.github.io\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>


<meta property="og:title" content="rinna 3.6bをdockerから使う" />
<meta property="og:description" content="モチベーション
日本語に大規模言語モデル（LLM）を試してみたくて、5月にリリースされたrinnaを使ってみた。インストールの手間を省くため、dockerコンテナ環境下でrinnaを実行した。
その際にハマったこともあったので以下にまとめる。">
<meta property="og:image" content="https://akenji3.github.io/img/avatar-icon.png" />
<meta property="og:url" content="https://akenji3.github.io/post/20230814_rinna36b/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="akenji&#39;s lab" />

  <meta name="twitter:title" content="rinna 3.6bをdockerから使う" />
  <meta name="twitter:description" content="モチベーション
日本語に大規模言語モデル（LLM）を試してみたくて、5月にリリースされたrinnaを使ってみた。インストールの手間を省くため、dockerコンテナ環境下でrinnaを実行した。
その際にハマったこともあったので以下にまとめる。">
  <meta name="twitter:image" content="https://akenji3.github.io/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='https://akenji3.github.io/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.147.1">
  <link rel="alternate" href="https://akenji3.github.io/index.xml" type="application/rss+xml" title="akenji&#39;s lab"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.6.0/css/all.css" integrity="sha384-h/hnnw1Bi4nbpD6kE7nYfCXzovi622sY5WBxww8ARKwpdLj5kUWjRuyiXaD1U2JT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="https://akenji3.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" />
  <link rel="stylesheet" href="https://akenji3.github.io/css/highlight.min.css" /><link rel="stylesheet" href="https://akenji3.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-TGXWYJXF48"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TGXWYJXF48');
        }
      </script>
  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">メニューを切り替え</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://akenji3.github.io/">akenji&#39;s lab</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Blog" href="/">Blog</a>
            </li>
          
        
          
            <li>
              <a title="About" href="/page/about/">About</a>
            </li>
          
        
          
            <li>
              <a title="Categories" href="/categories">Categories</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/tags">Tags</a>
            </li>
          
        

        
          
            <li>
              
                
                  <a href="https://akenji3.github.io/en/post/20230814_rinna36b/">en</a>
                
              
            </li>
          
        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="akenji&#39;s lab" href="https://akenji3.github.io/">
            <img class="avatar-img" src="https://akenji3.github.io/img/avatar-icon.png" alt="akenji&#39;s lab" />
           
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>rinna 3.6bをdockerから使う</h1>
              
              
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;August 14, 2023に投稿
  
  
  
  
    
      &nbsp;|&nbsp;<i class="fas fa-user"></i>&nbsp;
    
  
  &nbsp;&bull;&nbsp;翻訳：<a href="https://akenji3.github.io/en/post/20230814_rinna36b/" lang="en">en</a>
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <h2 id="モチベーション">モチベーション</h2>
<p>日本語に大規模言語モデル（LLM）を試してみたくて、5月にリリースされたrinnaを使ってみた。インストールの手間を省くため、dockerコンテナ環境下でrinnaを実行した。</p>
<p>その際にハマったこともあったので以下にまとめる。</p>
<h2 id="情報源">情報源</h2>
<ol>
<li><a href="https://rinna.co.jp/news/2023/05/20230507.html">rinnaのプレスリリース</a>  -  rinnaについてはここを参照。36億パラメータということ、今回試したrinnaを以下、rinna 3.6bと記述する。</li>
<li><a href="https://internet.watch.impress.co.jp/docs/column/shimizu/1503707.html">rinnaの紹介記事</a>  -  INTERNET Watchの記事。　サンプルコードもこの記事から流用した。</li>
<li><a href="https://qiita.com/Blaster36/items/246899ebe295d057b40b#fn-5">GPT日本語言語モデルを利用して映画「となりのトトロ」のストーリー概要を作成してみた（１本目）</a>  -  Pytorchが入っているコンテナイメージに関する内容は、この記事を参考にした。</li>
</ol>
<h2 id="rinna-36bの実行結果">rinna 3.6bの実行結果</h2>
<h3 id="実行するpythonコード">実行するpythonコード</h3>
<p>情報源2.のサンプルコードを${HOME}/workspace/rinna.pyとして保存する。</p>
<p>サンプルコードの内容は次のとおり。5番目のmodelのコメントを外して有効にした。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">import time
</span></span><span class="line"><span class="cl">import torch
</span></span><span class="line"><span class="cl">from transformers import AutoTokenizer, AutoModelForCausalLM
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">tokenizer = AutoTokenizer.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, use_fast=False)
</span></span><span class="line"><span class="cl"># 標準
</span></span><span class="line"><span class="cl">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;)
</span></span><span class="line"><span class="cl"># 自動
</span></span><span class="line"><span class="cl">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, device_map=&#39;auto&#39;)
</span></span><span class="line"><span class="cl"># 自動(VRAM16GB以下でも8GBはNG)
</span></span><span class="line"><span class="cl">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, torch_dtype=torch.float16, device_map=&#39;auto&#39;)
</span></span><span class="line"><span class="cl"># CPU指定
</span></span><span class="line"><span class="cl">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;).to(&#34;cpu&#34;)
</span></span><span class="line"><span class="cl"># GPU指定
</span></span><span class="line"><span class="cl">model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;).to(&#34;cuda&#34;)
</span></span><span class="line"><span class="cl"># GPU指定(VRAM16GB以下でも8GBはNG)
</span></span><span class="line"><span class="cl">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, torch_dtype=torch.float16).to(&#34;cuda&#34;)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">#繰り返し(抜けるのはCtl+c)
</span></span><span class="line"><span class="cl">prompt = &#34;&#34;
</span></span><span class="line"><span class="cl">while True:
</span></span><span class="line"><span class="cl">    # 質問を入力
</span></span><span class="line"><span class="cl">    question = input(&#34;質問をどうぞ： &#34;)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    if question.lower() == &#39;clear&#39;:
</span></span><span class="line"><span class="cl">        question = &#34;&#34;
</span></span><span class="line"><span class="cl">        prompt = &#34;&#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    prompt = prompt+f&#34;ユーザー: {question}&lt;NL&gt;システム: &#34;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    # 時間計測開始
</span></span><span class="line"><span class="cl">    start = time.time()
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=&#34;pt&#34;)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    with torch.no_grad():
</span></span><span class="line"><span class="cl">        output_ids = model.generate(
</span></span><span class="line"><span class="cl">            token_ids.to(model.device),
</span></span><span class="line"><span class="cl">            do_sample=True,
</span></span><span class="line"><span class="cl">            max_new_tokens=128,
</span></span><span class="line"><span class="cl">            temperature=0.7,
</span></span><span class="line"><span class="cl">            pad_token_id=tokenizer.pad_token_id,
</span></span><span class="line"><span class="cl">            bos_token_id=tokenizer.bos_token_id,
</span></span><span class="line"><span class="cl">            eos_token_id=tokenizer.eos_token_id
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">    output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])
</span></span><span class="line"><span class="cl">    output = output.replace(&#34;&lt;NL&gt;&#34;, &#34;\n&#34;)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    # 時間表示
</span></span><span class="line"><span class="cl">    end = time.time()
</span></span><span class="line"><span class="cl">    print(end-start)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">#    print(prompt)
</span></span><span class="line"><span class="cl">    print(output)
</span></span><span class="line"><span class="cl">    prompt = prompt+output+&#34;&lt;NL&gt;&#34;
</span></span></code></pre></div><h3 id="ngc-catalogからダウンロードコンテナ実行">NGC CATALOGからダウンロード、コンテナ実行</h3>
<p>情報源3.を参考に、<a href="https://catalog.ngc.nvidia.com/">NGC CATALOG</a>から<a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch">Pytorchコンテナ</a>をpull（ダウンロード）する。</p>
<p><a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch/tags">Catlog &gt; Containers &gt; Pytorchページ</a>のTagsを見ると、23.07-py3が最新なので、早速それをダウンロードして実行する。ユーザモードで動作するdockerについては<a href="https://akenji3.github.io/post/20230502_rootlessdocker/">このページ</a>を参考にして欲しい。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ mkdir <span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/workspace
</span></span><span class="line"><span class="cl">$ docker pull nvcr.io/nvidia/pytorch:23.07-py3
</span></span><span class="line"><span class="cl">$ docker run -d -it --gpus all --name gpt -v <span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/workspace:/workspace/local nvcr.io/nvidia/pytorch:23.07-py3
</span></span><span class="line"><span class="cl">$ docker <span class="nb">exec</span> -it gpt /bin/bash
</span></span></code></pre></div><h3 id="rtx-a4000での実行">RTX A4000での実行</h3>
<h4 id="docker環境下での操作">docker環境下での操作</h4>
<p>ここから上記で立ち上げたdockerコンテナでの操作する。</p>
<p>以下、#プロンプトから始まる行はコンテナ環境下での操作であること示す。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># python -m pip install transformers sentencepiece accelerate</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python rinna.py</span>
</span></span></code></pre></div><h4 id="deferredcudacallerror">DeferredCudaCallError</h4>
<p>上記pythonコードを実行中に次のようなエラーとなった。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">raise DeferredCudaCallError<span class="o">(</span>msg<span class="o">)</span> from e
</span></span><span class="line"><span class="cl">torch.cuda.DeferredCudaCallError: CUDA call failed lazily at initialization with error: device &gt;<span class="o">=</span> <span class="m">0</span> <span class="o">&amp;&amp;</span> device &lt; num_gpus INTERNAL ASSERT FAILED at <span class="s2">&#34;/opt/pytorch/pytorch/aten/src/ATen/cuda/CUDAContext.\
</span></span></span><span class="line"><span class="cl"><span class="s2">cpp&#34;</span>:50, please report a bug to PyTorch. <span class="nv">device</span><span class="o">=</span>1, <span class="nv">num_gpus</span><span class="o">=</span>
</span></span></code></pre></div><p>調べてみるとMIG設定時のエラーとのコメントもあった。自分のワークステーションは、A4000とQuadro K600の2つのGPUを装着している。このことから、使用するGPUを指定する必要があると考えて、次のとおり実行した。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># python -m pip install transformers sentencepiece accelerate</span>
</span></span><span class="line"><span class="cl"><span class="c1"># export CUDA_VISIBLE_DEVICES=&#34;0&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python rinna.py</span>
</span></span></code></pre></div><h4 id="実行結果">実行結果</h4>
<p>以下で「質問をどうぞ：」に続く文字列を入力した。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">You are using the legacy behaviour of the &lt;class <span class="s1">&#39;transformers.models.t5.tokenization_t5.T5Tokenizer&#39;</span>&gt;. This means that tokens that come after special tokens will not be properly handled. We recommend you to <span class="nb">read</span> the related pull request available at https://github.com/huggingface/transformers/pull/24565
</span></span><span class="line"><span class="cl">質問をどうぞ： 日本の終戦記念日はいつですか
</span></span><span class="line"><span class="cl">1.9004747867584229
</span></span><span class="line"><span class="cl">1945年8月15日です。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 江戸幕府を開いたのは誰ですか
</span></span><span class="line"><span class="cl">1.9013473987579346
</span></span><span class="line"><span class="cl">徳川家康は1603年に亡くなり、1604年に将軍になりました。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 大化の改新について教えてください
</span></span><span class="line"><span class="cl">5.68342661857605
</span></span><span class="line"><span class="cl">大化の改新は、天智天皇が日本の政治改革を行い、中央集権政府を確立した革命的な政治改革でした。この改革<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>は中央政府を強化し、律令制度を確立しました。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： ありがとうございました
</span></span><span class="line"><span class="cl">0.6675028800964355
</span></span><span class="line"><span class="cl">どういたしまして&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ：
</span></span></code></pre></div><p>江戸幕府の答えが少し変だが、細かいところは目をつぶる。</p>
<h3 id="titan-vでの実行">TITAN Vでの実行</h3>
<p>TITAN V（+Quadro K2000）を実装している別のワークステーションで同じことを実行した。</p>
<p>上記と同じように、NGC CATALOGからpytorchコンテナをダウンロードし、${HOME}/workspaceに格納しているrinna.pyを実行した。操作手順は次のとおり。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ docker run -d -it --gpus all --name gpt -v <span class="si">${</span><span class="nv">HOEM</span><span class="si">}</span>/workspace:/workspace/local <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>nvcr.io/nvidia/pytorch:23.07-py3
</span></span><span class="line"><span class="cl">$ docker <span class="nb">exec</span> -it gpt /bin/bash
</span></span><span class="line"><span class="cl"><span class="c1">## 以下はコンテナ内での操作</span>
</span></span><span class="line"><span class="cl"><span class="c1"># export CUDA_VISIBLE_DEVICES=&#34;0&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python -m pip install transformers sentencepiece accelerate</span>
</span></span><span class="line"><span class="cl"><span class="c1"># cd local</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python rinna.py</span>
</span></span></code></pre></div><h4 id="nvidia-driver-is-too-old">NVIDIA driver is too old</h4>
<p>上記を実行すると以下のエラーが発生。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">RuntimeError: The NVIDIA driver on your system is too old <span class="o">(</span>found version 11040<span class="o">)</span>. Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.
</span></span></code></pre></div><p>A4000では動いていた同じコンテナが、TITAN Vでは上記の通り「NVIDIA driver on your system is too old」との結果となり、理由が不明である。</p>
<p>NVIDIA driverが原因ではなく、pytorchがTITAN Vをサポートしていないことが原因だと想定し、古いバージョンのpytorchコンテナで試すことにした。</p>
<p>因みに 23.07-py3のpytorchコンテナのtorch関連のバージョンは次のとおりだった。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># pip list | grep torch</span>
</span></span><span class="line"><span class="cl">pytorch-quantization      2.1.2
</span></span><span class="line"><span class="cl">torch                     2.1.0a0+b5021ba
</span></span><span class="line"><span class="cl">torch-tensorrt            1.5.0.dev0
</span></span><span class="line"><span class="cl">torchdata                 0.7.0a0
</span></span><span class="line"><span class="cl">torchtext                 0.16.0a0
</span></span><span class="line"><span class="cl">torchvision               0.16.0a0
</span></span></code></pre></div><h4 id="1年前のpytorchコンテナで試す">1年前のpytorchコンテナで試す</h4>
<p>古いバージョンのpytorchコンテナで試してみようと思い、理由はないが、1年前の22.07-py3を次のようにして使ってみた。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">$ docker run -d -it --gpus all --name gpt -v /home/kenji/workspace:/workspace/local <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>nvcr.io/nvidia/pytorch:22.07-py3
</span></span><span class="line"><span class="cl">$ docker <span class="nb">exec</span> -it gpt /bin/bash
</span></span></code></pre></div><p>ここからは、コンテナ内の操作。</p>
<p>torchのバージョンは次のとおり1.13.0。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># pip list | grep torch</span>
</span></span><span class="line"><span class="cl">pytorch-quantization          2.1.2
</span></span><span class="line"><span class="cl">torch                         1.13.0a0+08820cb
</span></span><span class="line"><span class="cl">torch-tensorrt                1.2.0a0
</span></span><span class="line"><span class="cl">torchtext                     0.13.0a0
</span></span><span class="line"><span class="cl">torchvision                   0.14.0a0
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># export CUDA_VISIBLE_DEVICES=&#34;0&#34;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python -m pip install transformers sentencepiece accelerate</span>
</span></span><span class="line"><span class="cl"><span class="c1"># python rinna.py</span>
</span></span></code></pre></div><h4 id="cuda-out-of-memory">CUDA out of memory</h4>
<p>上記を実行すると、次のとおりGPUのVRAM容量不足となった。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl">RuntimeError: CUDA out of memory. Tried to allocate 122.00 MiB <span class="o">(</span>GPU 0<span class="p">;</span> 11.78 GiB total capacity<span class="p">;</span> 11.03 GiB already allocated<span class="p">;</span> 4.00 MiB free<span class="p">;</span> 11.12 GiB reserved in total by PyTorch<span class="o">)</span> If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation <span class="k">for</span> Memory Management and PYTORCH_CUDA_ALLOC_CONF
</span></span></code></pre></div><p>TITAN Vの12GBのVRAMでは足りないということなので、torch_dtypeを指定してモデル容量を小さくする。</p>
<p>具体的には、rinna.pyのmodelの5行目をコメントアウトし、6行目を有効にした。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># 標準
</span></span><span class="line"><span class="cl">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;)
</span></span><span class="line"><span class="cl"># 自動
</span></span><span class="line"><span class="cl">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, device_map=&#39;auto&#39;)
</span></span><span class="line"><span class="cl"># 自動(VRAM16GB以下でも8GBはNG)
</span></span><span class="line"><span class="cl">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, torch_dtype=torch.float16, device_map=&#39;auto&#39;)
</span></span><span class="line"><span class="cl"># CPU指定
</span></span><span class="line"><span class="cl">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;).to(&#34;cpu&#34;)
</span></span><span class="line"><span class="cl"># GPU指定
</span></span><span class="line"><span class="cl">#model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;).to(&#34;cuda&#34;)
</span></span><span class="line"><span class="cl"># GPU指定(VRAM16GB以下でも8GBはNG)
</span></span><span class="line"><span class="cl">model = AutoModelForCausalLM.from_pretrained(&#34;rinna/japanese-gpt-neox-3.6b-instruction-sft&#34;, torch_dtype=torch.float16).to(&#34;cuda&#34;)
</span></span></code></pre></div><h4 id="実行結果-1">実行結果</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-sh" data-lang="sh"><span class="line"><span class="cl"><span class="c1"># python rinna.py</span>
</span></span><span class="line"><span class="cl">You are using the legacy behaviour of the &lt;class <span class="s1">&#39;transformers.models.t5.tokenization_t5.T5Tokenizer&#39;</span>&gt;. This means that tokens that come after special tokens will not be properly handled. We recommend you to <span class="nb">read</span> the related pull request available at https://github.com/huggingface/transformers/pull/24565
</span></span><span class="line"><span class="cl">質問をどうぞ： 日本の終戦記念日はいつですか
</span></span><span class="line"><span class="cl">1.9047455787658691
</span></span><span class="line"><span class="cl">1945年8月15日です。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 江戸幕府を開いたのは誰ですか
</span></span><span class="line"><span class="cl">0.19092082977294922
</span></span><span class="line"><span class="cl">徳川家康&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 大化の改新について教えてください
</span></span><span class="line"><span class="cl">4.939814329147339
</span></span><span class="line"><span class="cl">大化の改新とは、中大兄皇子、後の天智天皇が政治改革を行った一連の改革です。これらの改革は、公正な競争<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>のルールや、身分制度にとらわれない人々の平等な権利の確立など、その後の日本の政治システムの基盤を提供<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>しました。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 南北朝時代になった原因を教えてください
</span></span><span class="line"><span class="cl">1.415543794631958
</span></span><span class="line"><span class="cl">歴史の授業で、南北朝時代がどのように起こったかを説明しているのですか?&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ： 南北朝時代がどうして起こったのか教えてください
</span></span><span class="line"><span class="cl">9.151806592941284
</span></span><span class="line"><span class="cl">それは複雑な過程を経ています。一般的には、中国が北朝と南朝に分裂したと言えますが、それだけではありま<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>せん。北朝は、13世紀後半に始まった第1次モンゴル軍の侵攻や、14世紀後半の明朝の拡大に対処しなければな<span class="se">\\</span>
</span></span><span class="line"><span class="cl">らなかったため、大きく揺れ動いています。一方、南朝は、14世紀半ばの明朝による中国再統一の結果、安定し<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>た状態にありました。しかしながら、清盛が平氏政権を樹立した後、北朝は弱体化し、最終的には明朝による統<span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>一によって終了することになりました。&lt;/s&gt;
</span></span><span class="line"><span class="cl">質問をどうぞ：
</span></span></code></pre></div><h2 id="まとめ">まとめ</h2>
<p>今回、dockerコンテナで実行するので、インストールに関連したトラブルは無く、簡単に実行できると思っていたが、上記のとおり、少しだけハマった。ハマったおかげで、docker操作や関連する知識が少しだけ吸収できた。</p>
<p>ハマったところは次のとおり。なお、原因についてはakenjiの想定であり、実際は違うかもしれない。</p>
<ul>
<li>DeferredCudaCallError
GPUを複数実装している場合に発生し得る。CUDA_VISIBLE_DEVICESで使用するGPUを明記することで解決した。</li>
<li>The NVIDIA driver on your system is too old
使用するpytorchコンテナが新しくて、GPUのアーキテクチャーをサポートしていない。古いpytorchコンテナを使う。
pytorchバージョンとサポートするsm（NVIDIA architecuter name）の関連表を知っている方、教えてほしい。</li>
<li>CUDA out of memory
良くあるGPUのVRAM不足。モデルサイズを小さくする。今回は、torch_dtype=torch.float16と指定した。</li>
</ul>

        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=https%3a%2f%2fakenji3.github.io%2fpost%2f20230814_rinna36b%2f&amp;text=rinna%203.6b%e3%82%92docker%e3%81%8b%e3%82%89%e4%bd%bf%e3%81%86&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fakenji3.github.io%2fpost%2f20230814_rinna36b%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=https%3a%2f%2fakenji3.github.io%2fpost%2f20230814_rinna36b%2f&amp;title=rinna%203.6b%e3%82%92docker%e3%81%8b%e3%82%89%e4%bd%bf%e3%81%86" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fakenji3.github.io%2fpost%2f20230814_rinna36b%2f&amp;title=rinna%203.6b%e3%82%92docker%e3%81%8b%e3%82%89%e4%bd%bf%e3%81%86" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=https%3a%2f%2fakenji3.github.io%2fpost%2f20230814_rinna36b%2f&amp;title=rinna%203.6b%e3%82%92docker%e3%81%8b%e3%82%89%e4%bd%bf%e3%81%86" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=https%3a%2f%2fakenji3.github.io%2fpost%2f20230814_rinna36b%2f&amp;description=rinna%203.6b%e3%82%92docker%e3%81%8b%e3%82%89%e4%bd%bf%e3%81%86" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://akenji3.github.io/post/20230709_ubuntu2204_install/" data-toggle="tooltip" data-placement="top" title="ubuntu 22.04 LTSをインストール">&larr; 前ページ</a>
            </li>
          
          
            <li class="next">
              <a href="https://akenji3.github.io/post/20230918_modulus_install/" data-toggle="tooltip" data-placement="top" title="NVIDIA Modulusを試す　〜　PINNsことはじめ">次ページ &rarr;</a>
            </li>
          
        </ul>
      


      

    </div>
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
          
          
          
        </ul>
        <p class="credits copyright text-muted">
          

          &nbsp;&bull;&nbsp;&copy;
          
            2026
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://akenji3.github.io/">akenji&#39;s lab</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          起動力に<a href="https://gohugo.io">Hugo v0.147.1</a> &nbsp;&bull;&nbsp; テーマに<a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a>に基づいている<a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="https://akenji3.github.io/js/main.js"></script>
<script src="https://akenji3.github.io/js/highlight.min.js"></script>
<script> hljs.initHighlightingOnLoad(); </script>
<script> $(document).ready(function() {$("pre.chroma").css("padding","0");}); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://akenji3.github.io/js/load-photoswipe.js"></script>










    
  </body>
</html>

